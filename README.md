# End-to-End Azure Data Engineering Project  ðŸš€  

A full-stack data engineering pipeline built with Azure Data Factory (ADF), Databricks, and Power BI. The solution ingests data, transforms it through a robust ETL/ELT flow, and ultimately produces dashboards/analytics for decision-making.  

## Overview  
This project implements an end-to-end data engineering workflow that:  
- Ingests raw data (can be from various sources: CSV, databases, streaming, etc.)  
- Processes/cleans/transforms data using Databricks notebooks  
- Loads transformed data into managed tables / data warehouse layers (e.g., bronze â†’ silver â†’ gold)  
- Provides visualization and analytics via Power BI dashboards  

It showcases how to combine cloud-native tools for scalable, maintainable, and production-ready data pipelines.  

## Architecture / Components  
| Component | Responsibility |
|----------|----------------|
| Azure Data Factory (ADF) | Orchestrates ingestion, scheduling, and data movements |
| Databricks | Data transformations: cleansing, aggregations, feature engineering |
| Data Warehouse / Lake (e.g. Azure Storage / Delta Lake) | Storage of raw and processed data (bronze/silver/gold layers) |
| Power BI | Dashboards and reporting layer â€” visualizes processed data for analytics & insights |

#  Architecture Diagram 
![Data Architecture](https://github.com/dosibhatlanirmalaaiswarya-bit/Azure-Based-ETL-Pipeline-Using-ADF-Databricks/blob/df76e10aa9acfe53cb2dec9966280151ac900df6/Azure%20Architecture%20Diagram.jpg?raw=true)

## Features  
- Modular ETL/ELT pipeline with layered data storage (raw â†’ processed â†’ analytics)  
- Scalable using cloud infrastructure (Azure + Databricks)  
- Clear orchestration of tasks (ingest, transform, load, visualize)  
- Suitable for batch or real-time (or scheduled) data workflows  
- Easy to extend: add new data sources, transformations, or dashboards  

## Prerequisites  
- Azure account with permissions to create ADF pipelines, Databricks workspace, and storage resources  
- Databricks runtime compatible with projectâ€™s code (e.g. Python/Scala/Spark version)  
- Power BI account (Pro or higher) for dashboard deployment  
- (Optional) Local development environment if using local notebooks or testing  
